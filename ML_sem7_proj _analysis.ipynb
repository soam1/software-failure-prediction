{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyNRAtxkPvQZE4YCmFe/XA4O"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Step 1: Load All Datasets**\n",
    "**Purpose**: In this step, we load the four datasets into separate Pandas DataFrames. This allows us to independently analyze and manipulate each dataset. By displaying the first few rows, we verify that the data has been loaded correctly and that we can view the structure of each dataset."
   ],
   "metadata": {
    "id": "N_5PBEjbcWjS"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4I7QWhnJcV2Q",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726165032856,
     "user_tz": -330,
     "elapsed": 1029,
     "user": {
      "displayName": "Sankalp Surya",
      "userId": "11437638315476356056"
     }
    },
    "outputId": "dff6befc-796e-4fba-996e-5fbd6eceffff",
    "ExecuteTime": {
     "end_time": "2024-09-15T07:12:53.224063Z",
     "start_time": "2024-09-15T07:12:52.085172Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the four datasets\n",
    "df1 = pd.read_csv('/content/heart_dataset_1.csv')\n",
    "df2 = pd.read_csv('/content/heart_dataset_2.csv')\n",
    "df3 = pd.read_csv('/content/heart_dataset_3.csv')\n",
    "df4 = pd.read_csv('/content/heart_dataset_4.csv')\n",
    "\n",
    "# Displaying first few rows of each dataset\n",
    "print(\"Dataset 1:\")\n",
    "print(df1.head())\n",
    "\n",
    "print(\"\\nDataset 2:\")\n",
    "print(df2.head())\n",
    "\n",
    "print(\"\\nDataset 3:\")\n",
    "print(df3.head())\n",
    "\n",
    "print(\"\\nDataset 4:\")\n",
    "print(df4.head())"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/heart_dataset_1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Load the four datasets\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m df1 \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/content/heart_dataset_1.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      5\u001B[0m df2 \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/content/heart_dataset_2.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m df3 \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/content/heart_dataset_3.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m    936\u001B[0m     dialect,\n\u001B[1;32m    937\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    944\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m    945\u001B[0m )\n\u001B[1;32m    946\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m--> 948\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    608\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    610\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 611\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m    613\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    614\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1445\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1447\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1448\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_engine(f, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1703\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1704\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1705\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m get_handle(\n\u001B[1;32m   1706\u001B[0m     f,\n\u001B[1;32m   1707\u001B[0m     mode,\n\u001B[1;32m   1708\u001B[0m     encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1709\u001B[0m     compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompression\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1710\u001B[0m     memory_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory_map\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m   1711\u001B[0m     is_text\u001B[38;5;241m=\u001B[39mis_text,\n\u001B[1;32m   1712\u001B[0m     errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding_errors\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstrict\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1713\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstorage_options\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1714\u001B[0m )\n\u001B[1;32m   1715\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1716\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:863\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    858\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    859\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    860\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    861\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    862\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 863\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[1;32m    864\u001B[0m             handle,\n\u001B[1;32m    865\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[1;32m    866\u001B[0m             encoding\u001B[38;5;241m=\u001B[39mioargs\u001B[38;5;241m.\u001B[39mencoding,\n\u001B[1;32m    867\u001B[0m             errors\u001B[38;5;241m=\u001B[39merrors,\n\u001B[1;32m    868\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    869\u001B[0m         )\n\u001B[1;32m    870\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    871\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    872\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/content/heart_dataset_1.csv'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Step 2: Check Data Quality for Each Dataset**\n",
    "**Purpose** : We check the structure of each dataset using info(). This step gives us an understanding of the number of columns, data types, and any missing values in each dataset. It's essential to know what data we're working with and whether any columns need special attention (e.g., missing or improperly formatted values)."
   ],
   "metadata": {
    "id": "E1mmwb78dWrH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Check dataset information for each dataset\n",
    "print(\"Dataset 1 Information:\")\n",
    "df1.info()\n",
    "\n",
    "print(\"\\nDataset 2 Information:\")\n",
    "df2.info()\n",
    "\n",
    "print(\"\\nDataset 3 Information:\")\n",
    "df3.info()\n",
    "\n",
    "print(\"\\nDataset 4 Information:\")\n",
    "df4.info()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zhpeHprUdBir",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726167451765,
     "user_tz": -330,
     "elapsed": 552,
     "user": {
      "displayName": "Sankalp Surya",
      "userId": "11437638315476356056"
     }
    },
    "outputId": "cb27dc84-90f5-48e7-f999-27ad86c18edd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Step 3: Check for Missing Values**\n",
    "**Purpose** : This step identifies any missing values in the datasets. Missing values can lead to erroneous results, so it's important to determine how many missing entries are present in each column of each dataset before proceeding with further analysis."
   ],
   "metadata": {
    "id": "bMcB0kUMebHv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Check for missing values in each dataset\n",
    "print(\"Missing values in Dataset 1:\")\n",
    "print(df1.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in Dataset 2:\")\n",
    "print(df2.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in Dataset 3:\")\n",
    "print(df3.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in Dataset 4:\")\n",
    "print(df4.isnull().sum())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3HS7hRHEeKu1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726165423124,
     "user_tz": -330,
     "elapsed": 1118,
     "user": {
      "displayName": "Sankalp Surya",
      "userId": "11437638315476356056"
     }
    },
    "outputId": "4aaf5c4c-8cf9-4d73-df39-40819b2de248"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Step 4: Handle Missing Values**\n",
    "Purpose: After identifying missing values, we handle them by filling in the missing values with the mean of the respective column. This is a standard approach to dealing with missing data, ensuring that we don't lose valuable data rows while avoiding introducing bias."
   ],
   "metadata": {
    "id": "BSxnZVqZe-iF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Fill missing values with the mean for each dataset\n",
    "df1.fillna(df1.mean(), inplace=True)\n",
    "df2.fillna(df2.mean(), inplace=True)\n",
    "df3.fillna(df3.mean(), inplace=True)\n",
    "df4.fillna(df4.mean(), inplace=True)\n",
    "\n",
    "# Verify missing values have been filled\n",
    "print(\"Missing values after handling in Dataset 1:\")\n",
    "print(df1.isnull().sum())\n",
    "print(\"Missing values after handling in Dataset 2:\")\n",
    "print(df2.isnull().sum())\n",
    "print(\"Missing values after handling in Dataset 3:\")\n",
    "print(df3.isnull().sum())\n",
    "print(\"Missing values after handling in Dataset 4:\")\n",
    "print(df4.isnull().sum())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xDMSXbI9egid",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726165722449,
     "user_tz": -330,
     "elapsed": 5,
     "user": {
      "displayName": "Sankalp Surya",
      "userId": "11437638315476356056"
     }
    },
    "outputId": "a5d97d34-1b04-4d01-8eab-8ec3ee14befd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Step 5: Check for Dataset Imbalance**\n",
    "Purpose: In this step, we check the distribution of the target variable in each dataset to see if there’s an imbalance. An imbalanced dataset (e.g., where one class is much larger than others) can affect model performance, so it’s important to know if our datasets are balanced or not."
   ],
   "metadata": {
    "id": "Cc0OY6dMfzAX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to check and visualize the distribution of all attributes in the dataset\n",
    "def check_and_visualize_all_attributes(df, dataset_num, axarr):\n",
    "    num_cols = len(df.columns)\n",
    "    num_rows = (num_cols + 3) // 4\n",
    "\n",
    "    for i, column in enumerate(df.columns):\n",
    "        ax = axarr[i // 4, i % 4]\n",
    "        if df[column].dtype == 'object' or df[column].nunique() < 10:\n",
    "            sns.countplot(x=column, data=df, ax=ax)\n",
    "            ax.set_title(f'{column} Count Distribution')\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_ylabel('Count')\n",
    "        else:\n",
    "            sns.histplot(df[column], kde=True, color='skyblue', ax=ax)\n",
    "            ax.set_title(f'{column} Histogram')\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_ylabel('Frequency')\n",
    "\n",
    "    for j in range(num_cols, axarr.size):\n",
    "        axarr[j // 4, j % 4].axis('off')\n",
    "\n",
    "# Check and visualize all attributes for each dataset\n",
    "for i, df in enumerate([df1, df2, df3, df4], start=1):\n",
    "    print(f\"\\nVisualizing attributes in Dataset {i}\")\n",
    "\n",
    "    num_cols = len(df.columns)\n",
    "    num_rows = (num_cols + 3) // 4\n",
    "    fig, axarr = plt.subplots(num_rows, 4, figsize=(25, 5 * num_rows), squeeze=False)\n",
    "\n",
    "    check_and_visualize_all_attributes(df, i, axarr)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZZw_mzCGfadh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726172370901,
     "user_tz": -330,
     "elapsed": 17349,
     "user": {
      "displayName": "Sankalp Surya",
      "userId": "11437638315476356056"
     }
    },
    "outputId": "b9eb78eb-e082-4739-a186-891ef6a95487"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Step 6: Generate Descriptive Statistics for Each Dataset**\n",
    "Purpose: Generating descriptive statistics helps us understand the basic statistical properties of the dataset, such as the mean, median, standard deviation, and percentiles. This gives us insights into the central tendencies and the spread of the data."
   ],
   "metadata": {
    "id": "To0YPi2JfR9n"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Generate descriptive statistics for each dataset\n",
    "print(\"\\nDescriptive Statistics for Dataset 1:\")\n",
    "print(df1.describe())\n",
    "\n",
    "print(\"\\nDescriptive Statistics for Dataset 2:\")\n",
    "print(df2.describe())\n",
    "\n",
    "print(\"\\nDescriptive Statistics for Dataset 3:\")\n",
    "print(df3.describe())\n",
    "\n",
    "print(\"\\nDescriptive Statistics for Dataset 4:\")\n",
    "print(df4.describe())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ruefbXlzfOSn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726165718500,
     "user_tz": -330,
     "elapsed": 1769,
     "user": {
      "displayName": "Sankalp Surya",
      "userId": "11437638315476356056"
     }
    },
    "outputId": "acd1a151-a2df-46b3-f2e9-4d68c68d7359"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Step 7: Visualize Outliers Using Boxplots**\n",
    "Purpose: Boxplots are useful for visually identifying outliers. Outliers are extreme values that can distort statistical analyses, so it’s crucial to detect and manage them. This step creates boxplots for each dataset, allowing us to see any values that are significantly higher or lower than the rest of the data."
   ],
   "metadata": {
    "id": "uXhREE6Yf9l5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create boxplots for all datasets\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Boxplots of Datasets', fontsize=16)\n",
    "\n",
    "# Plot boxplot for Dataset 1\n",
    "sns.boxplot(data=df1, ax=axs[0, 0])\n",
    "axs[0, 0].set_title('Dataset 1 Boxplot')\n",
    "axs[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot boxplot for Dataset 2\n",
    "sns.boxplot(data=df2, ax=axs[0, 1])\n",
    "axs[0, 1].set_title('Dataset 2 Boxplot')\n",
    "axs[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot boxplot for Dataset 3\n",
    "sns.boxplot(data=df3, ax=axs[1, 0])\n",
    "axs[1, 0].set_title('Dataset 3 Boxplot')\n",
    "axs[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot boxplot for Dataset 4\n",
    "sns.boxplot(data=df4, ax=axs[1, 1])\n",
    "axs[1, 1].set_title('Dataset 4 Boxplot')\n",
    "axs[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.3)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "aHMqUp_pf_rs",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726166433956,
     "user_tz": -330,
     "elapsed": 3587,
     "user": {
      "displayName": "Sankalp Surya",
      "userId": "11437638315476356056"
     }
    },
    "outputId": "74191294-b6c0-423f-db09-891ab4c3cb47"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Step 8: Remove Outliers Using the IQR Method**\n",
    "Purpose: Outliers are removed using the Interquartile Range (IQR) method. This method identifies and excludes values that lie outside the range of typical data points (beyond 1.5 times the IQR). Removing outliers can improve the accuracy of our models and prevent skewed analyses."
   ],
   "metadata": {
    "id": "999dRnHKivTi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare subplots for comparison\n",
    "fig, axs = plt.subplots(4, 2, figsize=(15, 20))  # 4 rows, 2 columns\n",
    "\n",
    "# Iterate over datasets to remove outliers and plot results\n",
    "for i, df in enumerate([df1, df2, df3, df4], start=1):\n",
    "    # Before removing outliers\n",
    "    sns.boxplot(data=df, ax=axs[i-1, 0])\n",
    "    axs[i-1, 0].set_title(f'Dataset {i} - Before Outlier Removal')\n",
    "    axs[i-1, 0].tick_params(axis='x', rotation=45)  # Rotate x-axis labels\n",
    "\n",
    "    # Calculate IQR and remove outliers\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df_cleaned = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "    # After removing outliers\n",
    "    sns.boxplot(data=df_cleaned, ax=axs[i-1, 1])\n",
    "    axs[i-1, 1].set_title(f'Dataset {i} - After Outlier Removal')\n",
    "    axs[i-1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "fkwKifkkgBQL",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726167630813,
     "user_tz": -330,
     "elapsed": 7253,
     "user": {
      "displayName": "Sankalp Surya",
      "userId": "11437638315476356056"
     }
    },
    "outputId": "728d93cb-597e-4a4e-df94-2e9d20a9c80a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Step 9: Identify the Distribution Pattern of Data**\n",
    "Purpose: In this step, we calculate the mean, median, and percentiles (Q1 and Q3) for each dataset. This helps us understand the distribution pattern of the data, such as whether it's normally distributed, skewed, or has a high level of variance. Understanding the data distribution is crucial for model selection and performance."
   ],
   "metadata": {
    "id": "C07DklBEjbOr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Iterate over datasets to calculate and plot mean, median, and percentiles\n",
    "for i, df in enumerate([df1, df2, df3, df4], start=1):\n",
    "    mean = df.mean()\n",
    "    median = df.median()\n",
    "    percentile_25 = df.quantile(0.25)\n",
    "    percentile_75 = df.quantile(0.75)\n",
    "\n",
    "    # Create a DataFrame for the statistics to plot\n",
    "    stats_df = pd.DataFrame({\n",
    "        'Mean': mean,\n",
    "        'Median': median,\n",
    "        '25th Percentile (Q1)': percentile_25,\n",
    "        '75th Percentile (Q3)': percentile_75\n",
    "    })\n",
    "\n",
    "    # Plot the statistics for the current dataset\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    stats_df.plot(kind='bar', figsize=(10, 6))\n",
    "    plt.title(f'Dataset {i} - Mean, Median, and Percentiles')\n",
    "    plt.ylabel('Values')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tYGsYavGi2Wb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726174152905,
     "user_tz": -330,
     "elapsed": 2946,
     "user": {
      "displayName": "Sankalp Surya",
      "userId": "11437638315476356056"
     }
    },
    "outputId": "e037cda0-03d4-4131-aa2e-f1edca909906"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Step 10: Calculate Trimmed Mean**\n",
    "Purpose: The trimmed mean is calculated by excluding a certain percentage of extreme values from both ends of the data. This is useful when we want a measure of central tendency that is not affected by outliers. The trimmed mean provides a more robust estimate of the true center of the data."
   ],
   "metadata": {
    "id": "GjlopRkKlLOT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import trim_mean\n",
    "\n",
    "# Trimming fraction (10%)\n",
    "trim_fraction = 0.1\n",
    "\n",
    "# Iterate over datasets to calculate trimmed mean and other statistics, and plot them\n",
    "for i, df in enumerate([df1, df2, df3, df4], start=1):\n",
    "    # Calculate statistics\n",
    "    mean = df.mean()\n",
    "    median = df.median()\n",
    "    percentile_25 = df.quantile(0.25)\n",
    "    percentile_75 = df.quantile(0.75)\n",
    "\n",
    "    # Calculate trimmed mean\n",
    "    trimmed_mean = trim_mean(df, proportiontocut=trim_fraction)\n",
    "\n",
    "    # Create a DataFrame for the statistics to plot\n",
    "    stats_df = pd.DataFrame({\n",
    "        'Mean': mean,\n",
    "        'Median': median,\n",
    "        'Trimmed Mean': trimmed_mean,\n",
    "        '25th Percentile (Q1)': percentile_25,\n",
    "        '75th Percentile (Q3)': percentile_75\n",
    "    })\n",
    "\n",
    "    # Plot the statistics for the current dataset\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    stats_df.plot(kind='bar', figsize=(10, 6))\n",
    "    plt.title(f'Dataset {i} - Mean, Trimmed Mean, Median, and Percentiles')\n",
    "    plt.ylabel('Values')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1642
    },
    "id": "8mJKhTD2jvM1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726167272155,
     "user_tz": -330,
     "elapsed": 3373,
     "user": {
      "displayName": "Sankalp Surya",
      "userId": "11437638315476356056"
     }
    },
    "outputId": "dd2c178a-439f-4dc9-ce54-5cecba8698cb"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Step 11: Correlation Analysis**\n",
    "Purpose: In this step, we perform correlation analysis to identify relationships between variables. A correlation matrix is created to show how strongly different variables are related to each other. This helps in understanding which features are most important and which ones are redundant."
   ],
   "metadata": {
    "id": "kPpNESAenOQS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a correlation matrix heatmap for each dataset\n",
    "for i, df in enumerate([df1, df2, df3, df4], start=1):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Generate a correlation matrix\n",
    "    corr_matrix = df.corr()\n",
    "\n",
    "    # Plot heatmap\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "    plt.title(f'Dataset {i} Correlation Matrix')\n",
    "\n",
    "    # Show the heatmap\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2225
    },
    "id": "6gaMRsa2lQSG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1726167800018,
     "user_tz": -330,
     "elapsed": 9477,
     "user": {
      "displayName": "Sankalp Surya",
      "userId": "11437638315476356056"
     }
    },
    "outputId": "be4cef67-d728-4e69-c035-bbfcc596dea0"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
